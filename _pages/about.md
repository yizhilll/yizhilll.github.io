---
layout: about
title: about
permalink: /
subtitle: <a href='#'>üíçüìñüíªüê∂‚õ∞Ô∏èü§ø</a>. # Address. Contacts. Moto. Etc.

profile:
  align: right
  image: turing_bench.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>w/ Jolly at the Alan Turing Memorial, Manchester</p>

selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: true
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

<meta name="google-site-verification" content="S4kbKtEbks2C_vUp5k0RsyUsqnr4iLwD6euFRIdAoQY" />


I am currently a Computer Science PhD candidate funded by the University of Manchester, supervised by [Prof. Chenghua Lin](https://chenghua-lin.github.io/). I am also a co-founder of the [Multimodal Art Projection (M-A-P)](https://m-a-p.ai) research community, which aims to drive open-source academia-level research to cutting-edge level as the industry. I've collaborated with [Dr. Jie Fu](https://bigaidream.github.io/) and had a lot fun.  

---
### Research

My current research focus is mainly about the the post-training of LLM with reinforce learning. Specifically, as we have stepped into the new era of training LLM with one/similar recipe by setting customized goals, how to train a real generalized model under such a paradigm?


Other research questions involving post-training of LLMs and multi-modal alignment:

* How to build an effective and robust self-evolved framework for LLMs with data synthesis (maingly during post-trianing)?
* How to unifiy the understanding and generation of vision-langauge models?
* The paradgim of aligning model among the text, vision and audio modalities.

Before the LLM era, my research interests could be concluded as these topics: language model evaluation, information retrieval, fairness in NLP, music modelling, and general topics natural language modelling.
More recent and detailed topics can be referred to my [publication pages](https://yizhilll.github.io/publications/).


---
### Passed Experience

* Intenrned at [J.P. Morgan Artificial Intelligence Research](https://www.jpmorgan.com/technology/artificial-intelligence).
* I previously worked as a research assistant at Tsinghua NLP Lab with [Prof. Zhiyuan Liu](https://nlp.csai.tsinghua.edu.cn/~lzy/).


Academic Service: reviewer at ACL, EACL, EMNLP, INLG, ISMIR, ICLR, ICASSP, NeurIPS.